{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Retrieve and Clean\n",
    "\n",
    "### This file can be run with \"run all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Legacy Store Details](#legacy-store-details)\n",
    "1. [Legacy Users](#legacy-users)\n",
    "1. [Orders Table](#orders-table)\n",
    "1. [Card Details](#card-details)\n",
    "1. [Store Details](#store-details)\n",
    "1. [Products Data](#products-data)\n",
    "1. [Date Events Data](#date-events-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALLWAYS RUN FIRST\n",
    "\n",
    "from database_utils import DatabaseConnector\n",
    "from data_extraction import DataExtraction\n",
    "from data_cleaning import DataCleaning\n",
    "import pandas as pd\n",
    "\n",
    "database_connector = DatabaseConnector()\n",
    "data_extractor = DataExtraction()\n",
    "data_cleaner = DataCleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connector.list_db_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy Store Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a database table as \"legacy_store_details_df\" and saves it as a csv\n",
    "legacy_store_details_df = data_extractor.read_rds_table(\"legacy_store_details\")\n",
    "legacy_store_details_df.to_csv(\"original_dfs/original_legacy_store_details_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints details of the \"legacy_store_details_df\"\n",
    "legacy_store_details_df.info()\n",
    "legacy_store_details_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans the legacy_store_details_data\n",
    "cleaned_legacy_store_details_df = legacy_store_details_df\n",
    "cleaned_legacy_store_details_df = cleaned_legacy_store_details_df.drop(\"lat\", axis=1)\n",
    "cleaned_legacy_store_details_df = data_cleaner.clean_null(cleaned_legacy_store_details_df)\n",
    "cleaned_legacy_store_details_df = cleaned_legacy_store_details_df[['address', 'longitude', 'latitude', 'locality', 'store_code', 'staff_numbers', 'opening_date', 'store_type', 'country_code', 'continent']]\n",
    "cleaned_legacy_store_details_df = data_cleaner.clean_rows_by_length_condition(cleaned_legacy_store_details_df, \"country_code\", 3)\n",
    "cleaned_legacy_store_details_df = data_cleaner.clean_convert_date_column(cleaned_legacy_store_details_df, \"opening_date\")\n",
    "cleaned_legacy_store_details_df = data_cleaner.clean_replace_value_in_column(cleaned_legacy_store_details_df, \"continent\", \"eeAmerica\", \"America\")\n",
    "cleaned_legacy_store_details_df = data_cleaner.clean_replace_value_in_column(cleaned_legacy_store_details_df, \"continent\", \"eeEurope\", \"Europe\")\n",
    "\n",
    "cleaned_legacy_store_details_df.info()\n",
    "cleaned_legacy_store_details_df.to_csv(\"cleaned_dfs/cleaned_legacy_store_details_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a database table as \"legacy_users_df\" and saves it as a csv\n",
    "legacy_users_df = data_extractor.read_rds_table(\"legacy_users\")\n",
    "legacy_users_df.to_csv(\"original_dfs/original_legacy_users_df.csv\")\n",
    "\n",
    "# Prints details of the \"legacy_users_df\"\n",
    "legacy_users_df.info()\n",
    "legacy_users_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts a new cleaned variable\n",
    "cleaned_legacy_users_df = legacy_users_df\n",
    "\n",
    "# Cleans the legacy_users_data\n",
    "cleaned_legacy_users_df = data_cleaner.clean_rows_by_length_condition(cleaned_legacy_users_df, \"country_code\", 3)\n",
    "cleaned_legacy_users_df = data_cleaner.clean_null(cleaned_legacy_users_df)\n",
    "cleaned_legacy_users_df = data_cleaner.clean_replace_value_in_column(cleaned_legacy_users_df, \"country_code\", \"GGB\", \"GB\")\n",
    "cleaned_legacy_users_df = data_cleaner.clean_convert_date_column(cleaned_legacy_users_df, \"date_of_birth\")\n",
    "cleaned_legacy_users_df = data_cleaner.clean_convert_date_column(cleaned_legacy_users_df, \"join_date\")\n",
    "# cleaned_legacy_users_df = cleaned_legacy_users_df.reset_index(drop=True)    # Resets the index\n",
    "\n",
    "cleaned_legacy_users_df.info()\n",
    "\n",
    "# Overwrites the cleaned_df.csv witht the current cleared_df \n",
    "cleaned_legacy_users_df.to_csv(\"cleaned_dfs/cleaned_legacy_users_df.csv\")\n",
    "\n",
    "# Uploads to sales_data\n",
    "database_connector.upload_to_db(cleaned_legacy_users_df, \"dim_users\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orders Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 120123 entries, 0 to 118804\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   level_0           120123 non-null  int64 \n",
      " 1   date_uuid         120123 non-null  object\n",
      " 2   first_name        15284 non-null   object\n",
      " 3   last_name         15284 non-null   object\n",
      " 4   user_uuid         120123 non-null  object\n",
      " 5   card_number       120123 non-null  int64 \n",
      " 6   store_code        120123 non-null  object\n",
      " 7   product_code      120123 non-null  object\n",
      " 8   1                 0 non-null       object\n",
      " 9   product_quantity  120123 non-null  int64 \n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 10.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'date_uuid', 'first_name', 'last_name', 'user_uuid',\n",
       "       'card_number', 'store_code', 'product_code', '1', 'product_quantity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reads a database table as \"orders_table_df\" and saves it as a csv\n",
    "orders_table_df = data_extractor.read_rds_table(\"orders_table\")\n",
    "orders_table_df.to_csv(\"original_dfs/original_orders_table_df.csv\")\n",
    "\n",
    "# Prints details of the \"orders_table_df\"\n",
    "orders_table_df.info()\n",
    "orders_table_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts a new cleaned variable\n",
    "cleaned_orders_table_df = orders_table_df\n",
    "\n",
    "# Cleans the orders_table_df\n",
    "cleaned_orders_table_df = cleaned_orders_table_df.drop([\"level_0\", \"1\", \"first_name\", \"last_name\"], axis=1)\n",
    "\n",
    "cleaned_orders_table_df.info()\n",
    "\n",
    "# Overwrites the cleaned_df.csv witht the current cleared_df \n",
    "cleaned_orders_table_df.to_csv(\"cleaned_dfs/cleaned_orders_table_df.csv\")\n",
    "\n",
    "# Uploads to sales_data\n",
    "database_connector.upload_to_db(cleaned_orders_table_df, \"orders_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Card Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from pdf and save to pdf\n",
    "card_details_df = data_extractor.retrieve_pdf_data()\n",
    "card_details_df.to_csv(\"original_dfs/original_card_details.csv\")\n",
    "\n",
    "# Prints details of the \"card_details_df\"\n",
    "card_details_df.info()\n",
    "card_details_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts a new cleaned variable\n",
    "cleaned_card_details_df = card_details_df\n",
    "\n",
    "# Cleans the \"card_details_df\"\n",
    "cleaned_card_details_df = data_cleaner.clean_null(cleaned_card_details_df)\n",
    "cleaned_card_details_df = data_cleaner.clean_rows_by_length_condition(cleaned_card_details_df, \"expiry_date\", 5)\n",
    "cleaned_card_details_df['expiry_date'] = pd.to_datetime(cleaned_card_details_df['expiry_date'], format='%m/%y')\n",
    "cleaned_card_details_df = data_cleaner.clean_convert_date_column(cleaned_card_details_df, \"date_payment_confirmed\")\n",
    "\n",
    "cleaned_card_details_df.info()\n",
    "\n",
    "# Overwrites the cleaned_df.csv with the current cleared_df \n",
    "cleaned_card_details_df.to_csv(\"cleaned_dfs/cleaned_card_details_df.csv\")\n",
    "\n",
    "# Uploads to sales_data\n",
    "database_connector.upload_to_db(cleaned_card_details_df, \"dim_card_details\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists number of stores in the store data\n",
    "data_extractor.list_number_of_stores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TAKES 10 MINMUTES!!!!\n",
    "store_data_df = data_extractor.retrieve_stores_data()\n",
    "store_data_df\n",
    "store_data_df.to_csv(\"original_dfs/original_store_data.csv\")\n",
    "\n",
    "# Note: This table appears to be the same as \"legacy_store_details_df\" although I have treated is is if it was not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 440 entries, 1 to 450\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   address        440 non-null    object        \n",
      " 1   longitude      440 non-null    object        \n",
      " 2   latitude       440 non-null    object        \n",
      " 3   locality       440 non-null    object        \n",
      " 4   store_code     440 non-null    object        \n",
      " 5   staff_numbers  440 non-null    object        \n",
      " 6   opening_date   440 non-null    datetime64[ns]\n",
      " 7   store_type     440 non-null    object        \n",
      " 8   country_code   440 non-null    object        \n",
      " 9   continent      440 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(9)\n",
      "memory usage: 37.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Starts a new cleaned variable\n",
    "cleaned_store_data_df = store_data_df\n",
    "\n",
    "# Cleans the store_data_df\n",
    "cleaned_store_data_df = cleaned_store_data_df.drop(\"lat\", axis=1)\n",
    "cleaned_store_data_df = data_cleaner.clean_null(cleaned_store_data_df)\n",
    "cleaned_store_data_df = cleaned_store_data_df[['address', 'longitude', 'latitude', 'locality', 'store_code', 'staff_numbers', 'opening_date', 'store_type', 'country_code', 'continent']]\n",
    "cleaned_store_data_df = data_cleaner.clean_rows_by_length_condition(cleaned_store_data_df, \"country_code\", 3)\n",
    "cleaned_store_data_df = data_cleaner.clean_convert_date_column(cleaned_store_data_df, \"opening_date\")\n",
    "cleaned_store_data_df = data_cleaner.clean_replace_value_in_column(cleaned_store_data_df, \"continent\", \"eeAmerica\", \"America\")\n",
    "cleaned_store_data_df = data_cleaner.clean_replace_value_in_column(cleaned_store_data_df, \"continent\", \"eeEurope\", \"Europe\")\n",
    "cleaned_store_data_df = data_cleaner.clean_replace_value_in_column(cleaned_store_data_df, \"staff_numbers\", \"30e\", 30)\n",
    "cleaned_store_data_df = data_cleaner.clean_replace_value_in_column(cleaned_store_data_df, \"staff_numbers\", \"A97\", 97)\n",
    "cleaned_store_data_df = data_cleaner.clean_replace_value_in_column(cleaned_store_data_df, \"staff_numbers\", \"80R\", 80)\n",
    "cleaned_store_data_df = data_cleaner.clean_replace_value_in_column(cleaned_store_data_df, \"staff_numbers\", \"J78\", 78)\n",
    "cleaned_store_data_df = data_cleaner.clean_replace_value_in_column(cleaned_store_data_df, \"staff_numbers\", \"3n9\", 39)\n",
    "\n",
    "cleaned_store_data_df.info()\n",
    "cleaned_store_data_df.to_csv(\"cleaned_dfs/cleaned_store_data_df.csv\")\n",
    "\n",
    "# Uploads to sales_data\n",
    "database_connector.upload_to_db(cleaned_store_data_df, \"dim_store_details\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Products Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads and saves the products data\n",
    "data_extractor.extract_from_s3()\n",
    "\n",
    "# prints details about the products data\n",
    "products_data_df = pd.read_csv(\"original_dfs/original_products.csv\")\n",
    "products_data_df.info()\n",
    "products_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts a new cleaned variable\n",
    "cleaned_products_data_df = products_data_df\n",
    "\n",
    "# cleans data in products_data_df\n",
    "cleaned_products_data_df = cleaned_products_data_df.drop(\"Unnamed: 0\", axis=1)\n",
    "cleaned_products_data_df = data_cleaner.clean_null(cleaned_products_data_df)\n",
    "cleaned_products_data_df = data_cleaner.clean_rows_by_length_condition(cleaned_products_data_df, \"product_price\", 7)\n",
    "cleaned_products_data_df = data_cleaner.clean_replace_value_in_column(cleaned_products_data_df, \"weight\", \"77g .\", \"77g\")\n",
    "cleaned_products_data_df = data_cleaner.clean_convert_date_column(cleaned_products_data_df, \"date_added\")\n",
    "\n",
    "# Apply the conversion function to the 'Weight' column\n",
    "cleaned_products_data_df['weight_in_kg'] = cleaned_products_data_df['weight'].apply(data_cleaner.convert_product_weight)\n",
    "\n",
    "cleaned_products_data_df.info()\n",
    "cleaned_products_data_df.to_csv(\"cleaned_dfs/cleaned_products_data_df.csv\")\n",
    "\n",
    "# Uploads to sales_data\n",
    "database_connector.upload_to_db(cleaned_products_data_df, \"dim_products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads JSON and writes to csv\n",
    "date_events_df = pd.read_json(\"https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json\")\n",
    "date_events_df.to_csv(\"original_dfs/original_date_events_df.csv\")\n",
    "\n",
    "# Print details on the \"date_events_df\"\n",
    "date_events_df.info()\n",
    "date_events_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts a new cleaned variable\n",
    "cleaned_date_events_df = date_events_df\n",
    "\n",
    "# cleans data in date_events_df\n",
    "cleaned_date_events_df = data_cleaner.clean_null(cleaned_date_events_df)\n",
    "cleaned_date_events_df = data_cleaner.clean_rows_by_length_condition(cleaned_date_events_df, \"month\", 3)\n",
    "cleaned_date_events_df = data_cleaner.concatenate_columns(cleaned_date_events_df, \"date\", [\"year\", \"month\", \"day\"], \"-\")\n",
    "cleaned_date_events_df = data_cleaner.concatenate_columns(cleaned_date_events_df, \"datetime\", [\"date\", \"timestamp\"], \" \")\n",
    "cleaned_date_events_df = data_cleaner.clean_convert_date_column(cleaned_date_events_df, \"date\")\n",
    "cleaned_date_events_df = data_cleaner.clean_convert_date_column(cleaned_date_events_df, \"datetime\")\n",
    "\n",
    "cleaned_date_events_df.info()\n",
    "cleaned_date_events_df.to_csv(\"cleaned_dfs/cleaned_date_events_df.csv\")\n",
    "\n",
    "# Uploads to sales_data\n",
    "database_connector.upload_to_db(cleaned_date_events_df, \"dim_date_time\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
